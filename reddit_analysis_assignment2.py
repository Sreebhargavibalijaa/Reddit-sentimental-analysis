# -*- coding: utf-8 -*-
"""reddit_analysis_assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EqMhWrJvKliLOZBmGZn-cFRxexloCsKk
"""

import csv
import dateutil
import gzip
import math
import numpy
import random
import sklearn
import string
import datetime
from sklearn import tree
from sklearn.model_selection import train_test_split

from collections import defaultdict
from gensim.models import Word2Vec
from nltk.stem.porter import *
from sklearn.manifold import TSNE
from sklearn import linear_model
import dateutil
def load_data():
    data = []

    label = ['image_id','unixtime','rawtime','title','total_votes','reddit_id','number_of_upvotes',\
    'subreddit','number_of_downvotes','localtime','score','number_of_comments','username',\
    'undefined1','undefined2', 'undefined3']

    with open('submissions.csv') as csvfile:
        csvReader = csv.reader(csvfile)
        for row in csvReader:
            if row[0] == '#image_id':
                continue
            d = {}
            for i,elem in enumerate(row):
                d[label[i]] = elem
            data.append(d)
    print(len(data))
    return data

#image.py
import json
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import os
IMAGE_DIR = 'image_feature'
PIC_DIR = 'pic'

def load_overall_stat():
    with open(os.path.join(IMAGE_DIR,'/content/overall_stat.json'),'r') as fp:
        overall_stat = json.load(fp)
    return overall_stat

def load_each_image_stat():
    with open(os.path.join(IMAGE_DIR,'/content/each_image_stat.json'),'r') as fp:
        each_image_stat = json.load(fp)
    return each_image_stat

def feature_extraction():
  for attribute in ["number_of_comments","total_votes"]:
    image_vec = {}
    each_image_stat = load_each_image_stat()
    overall_stat = load_overall_stat()

    avg_number_of_upvotes = overall_stat.get('avg_number_of_upvotes')
    std_number_of_upvotes = overall_stat.get('std_number_of_upvotes')
    avg_number_of_comments = overall_stat.get('avg_number_of_comments')
    std_number_of_comments = overall_stat.get('std_number_of_comments')

    for image_id,value in each_image_stat.items():
        if attribute == 'number_of_comments':
            image_vec[image_id] = [1, float(value.get('avg_number_of_upvotes')-avg_number_of_upvotes)/std_number_of_upvotes] # normalize
        elif attribute == 'total_votes':
            image_vec[image_id] = [1, float(value.get('avg_number_of_comments')-avg_number_of_comments)/std_number_of_comments]
        else:
            print ('{0} is undefined.'.format(attribute))
            break

    with open(os.path.join(IMAGE_DIR,'/content/image_vec_number_of_comments.json'),'w') as f:
        json.dump(image_vec,f)
def _image():
    data = load_data()
    data = data[:int(len(data)/3)]
    each_image_stat = {}

    features = ['number_of_upvotes','number_of_downvotes','number_of_comments']
    for record in data:
        image_id = record.get('image_id')
        if image_id not in each_image_stat:
            each_image_stat[image_id] = defaultdict(float)
        for feature in features:
            each_image_stat[image_id]['avg_'+feature] += float(record.get(feature))
        each_image_stat[image_id]['num_of_posts'] += 1
    
    for image_id,stat in each_image_stat.items():
        for feature in features:
            each_image_stat[image_id]['avg_'+feature] /= each_image_stat[image_id]['num_of_posts']
        each_image_stat[image_id] = dict(each_image_stat[image_id])
    
    number_of_upvotes = []
    for record in data:
     number_of_upvotes.append(int(record.get('number_of_upvotes')))
    number_of_downvotes = []
    for record in data:
     number_of_downvotes.append(int(record.get('number_of_downvotes')))
    number_of_comments = []
    for record in data:
     number_of_comments.append(int(record.get('number_of_comments')))


    overall_stat = {
        'avg_number_of_upvotes': np.mean(number_of_upvotes),
        'avg_number_of_downvotes': np.mean(number_of_downvotes),
        'avg_number_of_comments': np.mean(number_of_comments),
        'std_number_of_upvotes': np.std(number_of_upvotes),
        'std_number_of_downvotes': np.std(number_of_downvotes),
        'std_number_of_comments': np.std(number_of_comments)
    }

    with open(os.path.join(IMAGE_DIR,'/content/each_image_stat.json'),'w') as fp:
        json.dump(dict(each_image_stat),fp)
    with open(os.path.join(IMAGE_DIR,'/content/overall_stat.json'),'w') as fp:
        json.dump(overall_stat,fp)

 

if __name__ == '__main__':
    _image()
    feature_extraction()

#user_name
from collections import defaultdict
import json
import numpy as np
import matplotlib.pyplot as plt
import os
USER_DIR = 'user_feature'
PIC_DIR = 'pics'
def all_users_data():
    with open(os.path.join(USER_DIR,'/content/all_users_data.json'),'r') as f:
        all_users_data = json.load(f)
    return all_users_data

def _load_user_stat():
    with open(os.path.join(USER_DIR,'user_stat.json'),'r') as f:
        user_stat = json.load(f)
    return user_stat


def _username():
    data = load_data()
    data = data[:len(data)/3]
    user_post_num = defaultdict(int)
    user_down_vote = defaultdict(int)
    user_up_vote = defaultdict(int)
    user_vote = defaultdict(int)
    user_comment_num = defaultdict(int)

    for record in data:
        user_name = record.get('username')
        user_post_num[user_name] += 1
        user_comment_num[user_name] += int(record.get('number_of_comments'))
        user_down_vote[user_name] += int(record.get('number_of_downvotes'))
        user_up_vote[user_name] += int(record.get('number_of_upvotes'))
        user_vote[user_name] += int(record.get('total_votes'))

    post_num_list = sorted(user_post_num.items(),key=lambda item:item[1], reverse=True)
    avg_post_num = np.mean(user_post_num.values())
    median_post_num = np.median(user_post_num.values())
    std_post_num = np.std(user_post_num.values())

    avg_votes = np.mean(user_vote.values())
    std_votes = np.std(user_vote.values())

    avg_comments = np.mean(user_comment_num.values())
    std_comments = np.std(user_comment_num.values())

    def user_profile(user_name):
        profile = {
            'username':user_name,
            'num_of_post':user_post_num[user_name],
            'avg_comments':float(user_comment_num[user_name])/user_post_num[user_name],
            'avg_downvotes':float(user_down_vote[user_name])/user_post_num[user_name],
            'avg_upvotes':float(user_up_vote[user_name])/user_post_num[user_name],
            'avg_votes':float(user_vote[user_name])/user_post_num[user_name],
        }
        return profile

    each_user_profile = [user_profile(user) for user in user_post_num.keys()]

    result = {
        'top_10':[user_profile(ele[0]) for ele in post_num_list[:10]],
        'bottom_10':[user_profile(ele[0]) for ele in post_num_list[-10:]],
        'avg_post_num':avg_post_num,
        'median_post_num':median_post_num,
        'avg_votes':avg_votes,
        'std_votes':std_votes,
        'avg_comments':avg_comments,
        'std_comments':std_comments
    }

    with open(os.path.join(USER_DIR,'each_user_profile.json'),'w') as f:
        json.dump(each_user_profile,f)

    with open(os.path.join(USER_DIR,'user_stat.json'),'w') as f:
        json.dump(result,f)

    with open(os.path.join(USER_DIR,'all_user_post_num.json'),'w') as f:
        json.dump(dict(user_post_num),f)

def gen_feature(target):
    user_vec = {}
    user_stat = _load_user_stat()
    user_profile = all_users_data()
    total_avg_comments = user_stat['avg_comments']
    total_std_comments = user_stat['std_comments']
    total_avg_votes = user_stat['avg_votes']
    total_std_votes = user_stat['std_votes']
    
    for user in user_profile:
        user_name = user.get('username')
        avg_comments = user.get('avg_comments')
        avg_votes = user.get('avg_votes')
        if target == "number_of_comments":        
            user_vec[user_name] = [1,float(avg_votes-total_avg_votes)/total_std_votes]
        elif target == 'total_votes':
            user_vec[user_name] = [1,float(avg_comments-total_avg_comments)/total_std_comments]
        else:
            print('{0} is undefined.'.format(target))
            break

    with open(os.path.join(USER_DIR,'user_vec_'+target+'.json'),'w') as f:
        json.dump(user_vec,f)
def _evaluate():
    each_user_profile = all_users_data()
    post_list = []
    comment_list = []
    votes_list = []
    for user in each_user_profile:
        post_list.append(user.get('num_of_post'))
        comment_list.append(user.get('avg_comments'))
        votes_list.append(user.get('avg_votes')+ user.get('avg_comments'))
    
    result = {"post_num_vs_votes":np.corrcoef(post_list,votes_list).tolist(),
    "post_num_vs_comments": np.corrcoef(post_list,comment_list).tolist(),
    "votes_vs_comments":np.corrcoef(votes_list,comment_list).tolist()
    }
    with open(os.path.join(USER_DIR,'/content/correlation_matrix.json'),'w') as fp:
        json.dump(result,fp)

    plt.bar(comment_list,votes_list, color ='maroon',width = 0.4)

    plt.ylabel("average comments and votes (attention) of user")
    plt.xlabel("average number of the comments the user obtained")
    plt.title("votes VS comments")
    plt.xlim([0, 1500])     

if __name__ == '__main__':
    _evaluate()

# linear model.py
import csv
import nltk
nltk.download('stopwords')
data = []

label = ['image_id','unixtime','rawtime','title','total_votes','reddit_id','number_of_upvotes',\
'subreddit','number_of_downvotes','localtime','score','number_of_comments','username',\
'undefined1','undefined2', 'undefined3']

with open('submissions.csv', newline='', encoding='utf-8') as csvfile:
    csvReader = csv.reader(csvfile)
    for row in csvReader:
        if row[0] == '#image_id':
            continue
        d = {}
        for i,elem in enumerate(row):
            d[label[i]] = elem
        data.append(d)

# word count (title)
from collections import defaultdict
import string
wordCount = defaultdict(int)
punctuation = set(string.punctuation)

for d in data:
    r = ''.join([c for c in d['title'].lower() if not c in punctuation])
    for w in r.split():
        wordCount[w] += 1

print(len(wordCount))

from nltk.corpus import stopwords
for w in stopwords.words("english"):
    if w in wordCount:
        wordCount.pop(w)

print(len(wordCount))
counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()

# output top 50 frequent words in title
f = open('/content/top50_words_in_title.txt','w')

for x in range(50):
    f.write(counts[x][1] + ': ' + str(counts[x][0]) + '\n')

f.close()

# take top 1000 words to be our targets
words = [x[1] for x in counts[:1000]]
wordId = dict(zip(words, range(len(words))))
wordSet = set(words)

def feature(datum):
    feat = [0]*len(words)
    r = ''.join([c for c in datum['title'].lower() if not c in punctuation])
    for w in r.split():
        if w in words:
            feat[wordId[w]] += 1
    feat.append(1) #offset
    return feat

####unigrams
y_user_interaction = []
X=[]
for d in data:
#  if ((int(d['total_votes'])+int(d['number_of_comments'])) <=10000):
      X.append(feature(d))
      y_user_interaction.append(int(d['total_votes'])+int(d['number_of_comments']))
y_upvotes = [int(d['number_of_upvotes']) for d in data]
y_downvotes = [int(d['number_of_downvotes']) for d in data]
y_score = [int(d['score']) for d in data]
y_user_interaction = [int(d['total_votes'])+int(d['number_of_comments']) for d in data]
mean = sum(y_user_interaction)/len(y_user_interaction)
y_user_interaction = [(x/10000) for x in y_user_interaction] 
#normalization

Ntrain,Ntest = 100000,20000
Xtrain,Xtest = X[:Ntrain],X[Ntrain: Ntrain+Ntest]
ytrain,ytest = y_user_interaction[:Ntrain],y_user_interaction[Ntrain: Ntrain+Ntest]


from sklearn import linear_model
clf = linear_model.Ridge(1.0, fit_intercept=False)
clf.fit(Xtrain,ytrain)
theta = clf.coef_

#store word-theta pairs
f = open('model_coeeficients.txt','w')

for x in range(len(theta)):
    try:
        f.write(words[x] + ': ' + str(theta[x]) + '\n')
    except Exception as e:
        pass

f.close()
def MSE(predictions, labels):
    differences = [(x-y)**2 for x,y in zip(predictions,labels)]
    return sum(differences) / len(differences)
predictions = clf.predict(Xtest)
print(predictions)
print(y_user_interaction)
y = ytest
mse1 = MSE(y,predictions)
print(mse1)

#the 1,000 most common unigrams;
# (b) the 1,000 most common bigrams;
# (c) a model which uses a combination of unigrams and bigrams (i.e., some bigrams will be included if
# they are more popular than some unigrams, but the model dimensionality will still be 1,000).

bigramCount = defaultdict(int)
for d in data:
  r = ''.join([c for c in d['title'].lower() if not c in punctuation])
  words = r.split()
  for i in range(0,len(words)-1):
    bigram = words[i] + " " + words[i+1]
    bigramCount[bigram] += 1

countsBigram = [(bigramCount[d], d) for d in bigramCount.keys()]
countsBigram.sort()
countsBigram.reverse()
bigrams = [c[1] for c in countsBigram[:1000]]
bigramId = dict(zip(bigrams, range(len(bigrams))))
bigramSet = set(bigrams)
def feature_bigrams(text): #feature function for bigrams model

  feat = [0]*len(bigrams)
  words = text.split()
  for i in range(len(words)-1):
    bigram = words[i] + " " + words[i+1]
    try:
      feat[bigramId[bigram]] += 1
    except KeyError:
      continue
  feat.append(1) #offset
  return feat
''' 
a model which uses a combination of unigrams and bigrams (i.e., some bigrams will be included if
they are more popular than some unigrams, but the model dimensionality will still be 1,000).
'''
unigramCount = defaultdict(int)
for d in data:
  r = ''.join([c for c in d['title'].lower() if not c in punctuation])
  for w in r.split():
    #w = stemmer.stem(w) # with stemming
    unigramCount[w] += 1
countsUnigram = [(unigramCount[w], w) for w in unigramCount]
countsUnigram.sort()
countsUnigram.reverse()
unigrams = [x[1] for x in countsUnigram[:1000]]
unigramId = dict(zip(unigrams, range(len(unigrams))))
unigramSet = set(unigrams)
countsCombined = countsUnigram+ countsBigram
e=0
if(e<1):
 e=e+1
countsCombined.sort()
countsCombined.reverse()
combineds = [x[1] for x in countsCombined[:1000]]
combinedId = dict(zip(combineds, range(len(combineds))))


def feature_uni_bigrams(text):
  feat = [0]*len(combineds)
  words = text.split()
  for i in range(len(words)-1):
    bigram = words[i] + " " + words[i+1]
    try:
      feat[combinedId[bigram]] += 1
    except KeyError:
      continue
  for w in words:
    try:
      feat[combinedId[w]] += 1
    except KeyError:
      continue
  feat.append(1) #offset
  return feat

y_user_interaction = []
X=[]
reviewText = [''.join([c for c in datum['title'].lower() if not c in punctuation]) for datum in data]
for i in range(len(data)):
      X.append(feature_bigrams(reviewText[i]))
      y_user_interaction.append(int(d['total_votes'])+int(d['number_of_comments']))
# print(X[:5000])
y_upvotes = [int(d['number_of_upvotes']) for d in data]
y_downvotes = [int(d['number_of_downvotes']) for d in data]
y_score = [int(d['score']) for d in data]
y_user_interaction = [int(d['total_votes'])+int(d['number_of_comments']) for d in data]
mean = sum(y_user_interaction)/len(y_user_interaction)
y_user_interaction = [(x/10000) for x in y_user_interaction]

Ntrain,Ntest = 100000,20000
Xtrain,Xtest = X[:Ntrain],X[Ntrain: Ntrain+Ntest]
ytrain,ytest = y_user_interaction[:Ntrain],y_user_interaction[Ntrain: Ntrain+Ntest]


from sklearn import linear_model
clf = linear_model.Ridge(1.0, fit_intercept=False)
clf.fit(Xtrain,ytrain) # when using all set of data, MEMORY ERROR!!!
theta = clf.coef_

#store word-theta pairs
f = open('model_coeeficients.txt','w')

for x in range(len(theta)):
    try:
        f.write(words[x] + ': ' + str(theta[x]) + '\n')
    except Exception as e:
        pass

f.close()
def MSE(predictions, labels):
    differences = [(x-y)**2 for x,y in zip(predictions,labels)]
    return sum(differences) / len(differences)
predictions = clf.predict(Xtest)
print(predictions)
print(y_user_interaction)
y = ytest
mse1 = MSE(y,predictions)
print(mse1)

#model for combined unigrams and bigrams
y_user_interaction = []
X=[]
reviewText = [''.join([c for c in datum['title'].lower() if not c in punctuation]) for datum in data]
for i in range(len(data)):
      X.append(feature_uni_bigrams(reviewText[i]))
      y_user_interaction.append(int(d['total_votes'])+int(d['number_of_comments']))

y_upvotes = [int(d['number_of_upvotes']) for d in data]
y_downvotes = [int(d['number_of_downvotes']) for d in data]
y_score = [int(d['score']) for d in data]
y_user_interaction = [int(d['total_votes'])+int(d['number_of_comments']) for d in data]
mean = sum(y_user_interaction)/len(y_user_interaction)
y_user_interaction = [(x/10000) for x in y_user_interaction]

Ntrain,Ntest = 100000,20000
Xtrain,Xtest = X[:Ntrain],X[Ntrain: Ntrain+Ntest]
ytrain,ytest = y_user_interaction[:Ntrain],y_user_interaction[Ntrain: Ntrain+Ntest]

from sklearn import linear_model
clf = linear_model.Ridge(1.0, fit_intercept=False)
clf.fit(Xtrain,ytrain) # when using all set of data, MEMORY ERROR!!!
theta = clf.coef_

#store word-theta pairs
f = open('model_coeeficients.txt','w')

for x in range(len(theta)):
    try:
        f.write(words[x] + ': ' + str(theta[x]) + '\n')
    except Exception as e:
        pass

f.close()
def MSE(predictions, labels):
    differences = [(x-y)**2 for x,y in zip(predictions,labels)]
    return sum(differences) / len(differences)
predictions = clf.predict(Xtest)
mse1 = MSE(ytest,predictions)
print(mse1)
#Build linear regression class
import sklearn
model1 = sklearn.linear_model.LinearRegression(fit_intercept=False)
model1.fit(Xtrain,ytrain) # Train on first half
residuals1 = model1.predict(Xtest) # Test on second half
mse4 = MSE(ytest,predictions)
print(mse4)

#Build linear regression class
import sklearn
model1 = sklearn.linear_model.LinearRegression(fit_intercept=False)
model1.fit(Xtrain,ytrain) # Train on first half
residuals1 = model1.predict(Xtest) # Test on second half
mse4 = MSE(ytest,predictions)
print(mse4)

#mostCommonUnigrams
mostCommonUnigrams = []
wordSort = list(zip(theta[:-1], words))
wordSort.sort()
mostCommonUnigrams.append(wordSort)
print(mostCommonUnigrams)

#most popular bigrams
mostCommonBigrams = []
k = 10
countsBigram = [d for d in countsBigram[:k]]
mostCommonBigrams.append(countsBigram)
print(mostCommonBigrams)

#most common unigrams and bigrams
mostCommonBoth = []
k = 10
countsCombined = [d for d in countsCombined[:k]]
mostCommonBoth.append(countsCombined)

mostCommonBoth

def Cosine(x1,x2):
    numer = 0
    norm1 = 0
    norm2 = 0
    for a1,a2 in zip(x1,x2):
        numer += a1*a2
        norm1 += a1**2
        norm2 += a2**2
    if norm1*norm2:
        return numer / math.sqrt(norm1*norm2)
    return 0

import math
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in data: 
    r = ''.join([c for c in d['title'].lower() if not c in punctuation])
    for w in r.split():
        wordCount[w] += 1

counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()

words = [x[1] for x in counts[:1000]]

df = defaultdict(int)
for d in data: 
    r = ''.join([c for c in d['title'].lower() if not c in punctuation])
    for w in set(r.split()):
        df[w] += 1

rev = data[0] # Query review
tf = defaultdict(int)
r = ''.join([c for c in rev['title'].lower() if not c in punctuation])

for w in r.split():
    tf[w] = 1

tfidf = dict(zip(words,[tf[w] * math.log2(len(data) / df[w]) for w in words]))
tfidfQuery = [tf[w] * math.log2(len(data) / df[w]) for w in words]

similarities = []
for rev2 in data[1:]: 
    tf = defaultdict(int)
    r = ''.join([c for c in rev2['title'].lower() if not c in punctuation])
    for w in r.split():
        tf[w] = 1
    tfidf2 = [tf[w] * math.log2(len(data) / df[w]) for w in words]
    similarities.append((Cosine(tfidfQuery, tfidf2), rev2['title']))

similarities.sort(reverse=True)
sim, review = similarities[0]

#word2vec. #item2vec
beerStyles = {} # Style of each item
categories = set() # Set of item categories
reviewsPerUser = defaultdict(list)
reviewsPerscore = defaultdict(list)
beerIdToName = {} # Map an ID to the name of the product

data[0]

reviews = []
reviewDicts = []

for d in data:
    reviews.append(d['title'])
    beerStyles[d['image_id']] = d['subreddit']
    # categories.add(d['beer/style'])
    beerIdToName[d['image_id']] = d['subreddit']
    reviewsPerUser[d['username']].append((d['rawtime'], d['image_id']))
    reviewsPerscore[d['username']].append((d['rawtime'], d['image_id']))
    reviewDicts.append(d)

reviewTokens =[]
reviewLists = []

punctuation = set(string.punctuation)
for r in reviews:
    r = ''.join([c for c in r.lower() if not c in punctuation])
    tokens = []
    for w in r.split():
        tokens.append(w)
    reviewTokens.append(tokens)

    

for u in reviewsPerUser:
    rl = list(reviewsPerUser[u])
    rl.sort()
    reviewLists.append([x[1] for x in rl])

model = Word2Vec(reviewTokens,
                 min_count=2, # Words/items with fewer instances are discarded
                 size=10, # Model dimensionality
                 window=3, # Window size
                 sg=2) # Skip-gram model

model.wv.similar_by_word("table")
# with open(os.path.join(USER_DIR,'word_2_vec__word(table).json'),'w') as f:
#         json.dump(word_2_vec__word(table),f)

X = model[model.wv.vocab]
from nltk.cluster import KMeansClusterer
import nltk
NUM_CLUSTERS=4
kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)
assigned_clusters = kclusterer.cluster(X, assign_clusters=True)
print (assigned_clusters)

from sklearn import cluster
from sklearn import metrics
kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)
kmeans_model = kmeans.fit(X)
 
labels = kmeans.labels_
centers = np.array(kmeans.cluster_centers_)
 
print ("Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):")
print (kmeans.score(X))
 
silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')
 
print ("Silhouette_score: ")
print (silhouette_score)

from collections import defaultdict
clusters = defaultdict(list)
words = list(model.wv.vocab)
for i, word in enumerate(words):  
   clusters[assigned_clusters[i]].append(word)

import numpy as np

data = [clusters[1],clusters[2],clusters[3],clusters[4]]

np.savetxt("word_clusters.csv", data, delimiter=", ", fmt="% s")

#Item2vec model building
#Almost the same as word2vec, but "documents" are made up of sequences of item IDs rather than words


reviewLists = []
for u in reviewsPerUser:
    rl = list(reviewsPerUser[u])
    rl.sort()
    reviewLists.append([x[1] for x in rl])
model10 = Word2Vec(reviewLists,
                 min_count=5, # Words/items with fewer instances are discarded
                 size=10, # Model dimensionality
                 window=3, # Window size
                 sg=1) # Skip-gram model

image_id = data[0]["image_id"]
similarities =  model10.wv.similar_by_word(image_id)[:5]
print(similarities)

X = []
beers = []
for b in beerIdToName:
    try:
        X.append(list(model10.wv[b]))
        beers.append(b)
    except Exception as e:
        pass

X_embedded = TSNE(n_components=2).fit_transform(X)
beers

X_embedded.shape

a=[]
X_embedded
print(len(X_embedded))
for d in data:
   a.append(d['image_id'])

len(set(a))

# Fit and transform
# Create DF
import pandas as pd
embeddingsdf = pd.DataFrame()
embeddingsdf['x'] = X_embedded[:,0]
# Add y coordinate
embeddingsdf['y'] = X_embedded[:,1]
# Check
embeddingsdf.head()

# Set figsize
fig, ax = plt.subplots(figsize=(10,8))
# Scatter points, set alpha low to make points translucent
ax.scatter(embeddingsdf.x, embeddingsdf.y, alpha=.1)
plt.title('Scatter plot of images using t-SNE')
plt.show()

"""**Predict** the rating using item2vec item similarity scores"""

ratingMean = sum([(int(d['score'])+int(d['total_votes']))/10000 for d in reviewDicts]) / len(reviewDicts)
ratingMean

itemAverages = defaultdict(list)
reviewsPerUser = defaultdict(list)
    
for d in reviewDicts:
    i = d['image_id']
    u = d['username']
    itemAverages[i].append((int(d['score'])+int(d['total_votes']))/10000)
    reviewsPerUser[u].append(d)
    
for i in itemAverages:
    itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])

def predictRating(user,item):
    ratings = []
    similarities = []
    if not str(item) in model10.wv:
        return ratingMean
    for d in reviewsPerUser[user]:
        i2 = d['image_id']
        if i2 == item: continue
        ratings.append((int(d['score'])+int(d['total_votes']))/10000 - itemAverages[i2])
        if str(i2) in model10.wv:
            similarities.append(model10.wv.distance(str(item), str(i2)))
    if (sum(similarities) > 0):
        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]
        return itemAverages[item] + sum(weightedRatings) / sum(similarities)
    else:
        return ratingMean

u,i = reviewDicts[0]['username'],reviewDicts[0]['image_id']

predictRating(u,i)
alwaysPredictMean = [ratingMean for _ in reviewDicts[:50]]
labels = [(int(d['score'])+int(d['total_votes']))/10000 for d in reviewDicts[:50]]
predictions = [predictRating(d['username'],d['image_id']) for d in reviewDicts[:50]]
def MSE(predictions, labels):
    differences = [(x-y)**2 for x,y in zip(predictions,labels)]
    return sum(differences) / len(differences)

predictions

# MSE(predictions, labels)
labels
MSE(predictions, labels)

"""Use item2vec to make recommendations

> Indented block


"""

############Use item2vec to make recommendations
def recScore(i, userHistory):
    historyInVocab = [w for w in userHistory if w in model10.wv]
    if len(historyInVocab) == 0:
        return 0
    sc = model10.wv.distance(str(i), historyInVocab[-1])
    return sc

def rec(userHistory):
    historyInVocab = [w for w in userHistory if w in model10.wv]
    if len(historyInVocab) == 0:
        return 0
    return model10.wv.most_similar(positive = historyInVocab, topn=10)

recScore(20539, reviewLists[0])

#Simple sentiment analysis pipeline
def feature(datum, words, wordId, tolower=True, removePunct=True):
    feat = [0]*len(words)
    r = datum['title']
    if tolower:
        r = r.lower()
    if removePunct:
        r = ''.join([c for c in r if not c in punctuation])
    for w in r.split():
        if w in words:
            feat[wordId[w]] += 1
    feat.append(1) # offset
    return feat

def pipeline(dSize = 1000, tolower=True, removePunct=True):
    wordCount = defaultdict(int)
    punctuation = set(string.punctuation)
    for d in data: # Strictly, should just use the *training* data to extract word counts
        r = d['title']
        if tolower:
            r = r.lower()
        if removePunct:
            r = ''.join([c for c in r if not c in punctuation])
        for w in r.split():
            wordCount[w] += 1

    counts = [(wordCount[w], w) for w in wordCount]
    counts.sort()
    counts.reverse()
    
    words = [x[1] for x in counts[:dSize]]
    
    wordId = dict(zip(words, range(len(words))))
    wordSet = set(words)
    
    X = [feature(d, words, wordId, tolower, removePunct) for d in data]
    y = [(int(d['score'])+int(d['number_of_comments']))/1000 for d in data]
    
    Ntrain,Nvalid,Ntest = 4000,500,500
    Xtrain,Xvalid,Xtest = X[:Ntrain],X[Ntrain:Ntrain+Nvalid],X[Ntrain+Nvalid:]
    ytrain,yvalid,ytest = y[:Ntrain],y[Ntrain:Ntrain+Nvalid],y[Ntrain+Nvalid:]
    
    bestModel = None
    bestVal = None
    bestLamb = None
    
    ls = [0.01, 0.1, 1, 10, 100, 1000, 10000]
    errorTrain = []
    errorValid = []

    for l in ls:
        model = sklearn.linear_model.Ridge(l)
        model.fit(Xtrain, ytrain)
        predictTrain = model.predict(Xtrain)
        MSEtrain = sum((ytrain - predictTrain)**2)/len(ytrain)
        errorTrain.append(MSEtrain)
        predictValid = model.predict(Xvalid)
        MSEvalid = sum((yvalid - predictValid)**2)/len(yvalid)
        errorValid.append(MSEvalid)
        print("l = " + str(l) + ", validation MSE = " + str(MSEvalid))
        if bestVal == None or MSEvalid < bestVal:
            bestVal = MSEvalid
            bestModel = model
            bestLamb = l
            
    predictTest = bestModel.predict(Xtest)
    MSEtest = sum((ytest - predictTest)**2)/len(ytest)
    MSEtest
    
    plt.xticks([])
    plt.xlabel(r"$\lambda$")
    plt.ylabel(r"error (MSE)")
    plt.title(r"Validation Pipeline")
    plt.xscale('log')
    plt.plot(ls, errorTrain, color='k', linestyle='--', label='training error')
    plt.plot(ls, errorValid, color='grey',zorder=4,label="validation error")
    plt.plot([bestLamb], [MSEtest], linestyle='', marker='x', color='k', label="test error")
    plt.legend(loc='best')
    plt.show()

pipeline(2000, False, False)

from collections import defaultdict
import json
import numpy 
import matplotlib.pyplot as plt
import os
from math import log

data=load_data()
unixtime=[]
upvotes=[]
downvotes=[]
comments=[]
score=[]
features=[]
votes=[]
user=[]
image=[]
isfunny=[]
isgif=[]
localtime=[]
length=len(data)
title=[]
time1=[]
v1=[]
time10005=[]
v2=[]
time1001=[]
v3=[]
community=defaultdict(list)
index=0

for record in data:
	unixtime.append(float(record.get('unixtime')))
	upvotes.append(float(record.get('number_of_upvotes')))
	downvotes.append(float(record.get('number_of_downvotes')))
	comments.append(float(record.get('number_of_comments')))
	score.append(float(record.get('score')))
	votes.append(float(record.get('total_votes')))
	user.append(record.get('username'))
	image.append(record.get('image_id'))
	title.append(record.get('title'))

	community[record.get('subreddit')].append(index)

	if record.get('subreddit')=='funny':
		isfunny.append(1)
	else:
		isfunny.append(0)
	if record.get('subreddit')=='gif':
		isgif.append(1)
	else:
		isgif.append(0)
	t=record.get('rawtime')
	te=t.split('-')
	temp=[]
	temp.append(int(te[0]))
	temp.append(int(te[1]))
	day=te[2].split('T')

	localtime.append([int(te[0]),int(te[1]),int(day[0])])

year=2012

day=3 #5,6,13

test=[]
userdetect=[]
timedetect=[]
noone=[]
imagedetect=defaultdict(int)
length=len(data)
timedic=defaultdict(int)
yeartime1=defaultdict(int)
yeartime2=defaultdict(int)
yeartime3=defaultdict(int)

for i in range(length):
	test.append((upvotes[i],title[i],user[i],localtime[i],isfunny[i],isgif[i]))
	userdetect.append((upvotes[i],user[i]))
	if localtime[i][0]==2010:
		yeartime1[localtime[i][1]]+=upvotes[i]
	if localtime[i][0]==2011:
		yeartime2[localtime[i][1]]+=upvotes[i]
	if localtime[i][0]==2012:
		yeartime3[localtime[i][1]]+=upvotes[i]
	if user[i]=='':
		noone.append(upvotes[i])
	timedic[(localtime[i][0],localtime[i][1],localtime[i][2])]+=1
	imagedetect[image[i]]+=1
	timedetect.append((unixtime[i],upvotes[i]))
timedetect.sort()
x=[]
y=[]
for (ti,up) in timedetect:
	x.append(ti)
	y.append(up)

print(x)

plt.plot(x,y)
plt.xlabel("time")
plt.ylabel("Upvotes")

plt.show()
noone.sort()
noone.reverse()

##plot for each weekday

from collections import defaultdict
import json
import numpy 
import matplotlib.pyplot as plt
import os
from math import log

data=load_data()
# unixtime=[]
year = ['2010','2011','2012']
for x in range(0,3):   
  rows = 7
  cols = 12
  
  m = [[0 for _ in range(cols)] for _ in range(rows)]
  for record in data:
    a = record.get('rawtime')
    if(a[0:4] == year[x]):
      i = int(a[5:7])-1
      if (record.get('subreddit')=='funny'):
        m[1][i] = m[1][i] + int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
      elif (record.get('subreddit')=='gif'):
        m[2][i] = m[2][i] + int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
      elif (record.get('subreddit')=='gaming'):
        m[3][i] = m[3][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
      elif (record.get('subreddit')=='atheism'):
        m[4][i] = m[4][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
      elif (record.get('subreddit')=='pics'):
        m[5][i] = m[5][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
      else:
        m[6][i] = m[6][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))

    
  barWidth = 0.25
  fig = plt.subplots(figsize =(12, 8))

  br1 = np.arange(12)
  br2 = [x  for x in br1]
  br3 = [x  for x in br2]
  br4 = [x  for x in br3]
  br5 = [x  for x in br4]
  br6 = [x  for x in br5]
  
  # Make the plot
  plt.bar(br1, m[1], color ='g', width = barWidth, edgecolor ='grey', label ='Funny')
  plt.bar(br2, m[2], color ='y', width = barWidth, edgecolor ='yellow', label ='Gif')
  plt.bar(br3, m[3], color ='b', width = barWidth,edgecolor ='black', label ='gaming')
  plt.bar(br4, m[4], color ='r', width = barWidth, edgecolor ='red', label ='atheism')
  plt.bar(br5, m[5], color ='b', width = barWidth, edgecolor ='blue', label ='pics')
  plt.bar(br6, m[6], color ='cyan', width = barWidth, edgecolor ='cyan', label ='Others')
  
  # Adding Xticks
  plt.xlabel('Month', fontweight ='bold', fontsize = 15)
  plt.ylabel('Attention (Total votes +comments), '+ year[x], fontweight ='bold', fontsize = 15)
  plt.xticks([r for r in range(12)],["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul","Aug", "Sep", "Oct", "Nov", "Dec"],fontsize = 15)
  
  plt.legend()
  print(year[x])
  if(year[x]=='2010'):
    plt.savefig('fig1.png')
  elif(year[x]=='2011'):
    plt.savefig('2011_1.png')
  else:
    plt.savefig('2012_1.png')
  plt.show()

#plot for each hour of a day
rows = 7
cols = 24
l=[]
m = [[0 for _ in range(cols)] for _ in range(rows)]
for record in data:
  a = record.get('rawtime')
  i = int(a[11:13])-1
  if (record.get('subreddit')=='funny'):
    m[1][i] = m[1][i] + int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='gif'):
    m[2][i] = m[2][i] + int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='gaming'):
    m[3][i] = m[3][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='atheism'):
    m[4][i] = m[4][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='pics'):
    m[5][i] = m[5][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  else:
    l.append(record.get('subreddit'))
    m[6][i] = m[6][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))

barWidth = 0.25
fig = plt.subplots(figsize =(24, 8))

br1 = np.arange(24)
br2 = [x  for x in br1]
br3 = [x  for x in br2]
br4 = [x  for x in br3]
br5 = [x  for x in br4]
br6 = [x  for x in br5]
 
# Make the plot
plt.bar(br1, m[1], color ='g', width = barWidth, edgecolor ='grey', label ='Funny')
plt.bar(br2, m[2], color ='y', width = barWidth, edgecolor ='yellow', label ='Gif')
plt.bar(br3, m[3], color ='b', width = barWidth,edgecolor ='black', label ='gaming')
plt.bar(br4, m[4], color ='r', width = barWidth, edgecolor ='red', label ='atheism')
plt.bar(br5, m[5], color ='b', width = barWidth, edgecolor ='blue', label ='pics')
plt.bar(br6, m[6], color ='y', width = barWidth, edgecolor ='yellow', label ='Others')
 
# Adding Xticks
plt.xlabel('time (UTC)', fontweight ='bold', fontsize = 20)
plt.ylabel('Attention and engagament', fontweight ='bold', fontsize = 20)
plt.xticks([r for r in range(24)],["12 AM","1 AM","2 AM","3 AM","4 AM","5 AM","6 AM","7 AM","8 AM","9 AM","10 AM","11 AM","12 PM","1 PM","2 PM","3 PM","4 PM","5 PM","6 PM","7 PM","8 PM","9 PM","10 PM","11 PM"], fontsize = 15)
plt.title("Average success over time")
plt.legend()
plt.show()

#plotting for day of a week
rows = 7
cols = 7
def convertNumToDay(num):
 weekdays = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
 return weekdays[num-1]
m = [[0 for _ in range(cols)] for _ in range(rows)]
for record in data:
  a = record.get('rawtime')
  today = datetime.datetime(int(a[:4]), int(a[5:7]), int(a[8:10]),int(a[11:13]),int(a[14:16]),int(a[17:19]))
  i = (today.weekday())
  if (record.get('subreddit')=='funny'):
    m[1][i] = m[1][i] + int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='gif'):
    m[2][i] = m[2][i] + int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='gaming'):
    m[3][i] = m[3][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='atheism'):
    m[4][i] = m[4][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  elif (record.get('subreddit')=='pics'):
    m[5][i] = m[5][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))
  else:
    m[6][i] = m[6][i] +  int(record.get('number_of_comments'))+ int(record.get('number_of_upvotes'))

barWidth = 0.25
fig = plt.subplots(figsize =(7, 8))

br1 = np.arange(7)
br2 = [x  for x in br1]
br3 = [x  for x in br2]
br4 = [x  for x in br3]
br5 = [x  for x in br4]
br6 = [x  for x in br5]
 
# Make the plot
plt.bar(br1, m[1], color ='g', width = barWidth, edgecolor ='grey', label ='Funny')
plt.bar(br2, m[2], color ='b', width = barWidth, edgecolor ='blue', label ='Gif')
plt.bar(br3, m[3], color ='b', width = barWidth,edgecolor ='black', label ='gaming')
plt.bar(br4, m[4], color ='r', width = barWidth, edgecolor ='red', label ='atheism')
plt.bar(br5, m[5], color ='b', width = barWidth, edgecolor ='blue', label ='pics')
plt.bar(br6, m[6], color ='y', width = barWidth, edgecolor ='yellow', label ='Others')
 
# Adding Xticks
plt.xlabel('time (UTC)', fontweight ='bold', fontsize = 15)
plt.ylabel('Attention and engagament', fontweight ='bold', fontsize = 15)
plt.xticks([r for r in range(7)],['Sun', 'Mon', 'Tues', 'Wed', 'Thur', 'Fri', 'Sat'],fontsize = 20)
plt.title("Average success over time")
plt.legend()
plt.show()